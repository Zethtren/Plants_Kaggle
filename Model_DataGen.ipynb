{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Strategy and model:\n",
    "\n",
    "For simplicity sake I am using a pretrained ResNet152 model.\n",
    "\n",
    "You will get faster training times using a smaller model but typically see better accuracy with larger models. \n",
    "\n",
    "The point of this exercise is to train a model using multiple GPU's on the google cloud. So I made sure to use a model structure where this was necessary to see improved results.\n",
    "\n",
    "\n",
    "### Mirror Strategy:\n",
    "\n",
    "The TensorFlow Distributed library comes with several distribution options. \n",
    "\n",
    "The Mirrored strategy allows for training on multiple GPU's the gradients are then calculated through all-reduce.\n",
    "\n",
    "\n",
    "See the TensorFlow Documentation for more information.\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/guide/distributed_training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirrored_strategy = tensorflow.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\",\"/gpu:3\"])\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "# When working with TensorFlow Distributed the model needs to be built inside the scope.\n",
    "# Since this notebook was written for the use of 4x NVIDIA P100's I have specified each GPU by it's label\n",
    "# Inside the scope below I am importing a pretrained ResNet152 Architecture and replacing it's final layer\n",
    "# I am then setting all of the layers to trainable. (Again there are better ways to practice this I did so for simplicity).\n",
    "# After the model is built you compile it. Since the DataSet has four categoried I am using Categorical Crossentropy and Adam\n",
    "\n",
    "####\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    resnet_base = ResNet152()\n",
    "    layers = [l for l in resnet_base.layers]\n",
    "    new_out = Dense(4, activation='softmax')(layers[-2].output)\n",
    "    resmodel = Model(inputs= resnet_base.input, outputs=new_out)\n",
    "    \n",
    "    for layer in resmodel.layers:\n",
    "        layer.trainable=True\n",
    "    resmodel.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGens:\n",
    "Since we have our Data stored in DataFrames I am creating a Train and Validation Generator. Only a Train and Test set were provided so I will start by splitting off a portion for Validation Data. \n",
    "\n",
    "When working with image Data (Or most data really..) it is considered a best practice to rescale to between 0 and 1.\n",
    "\n",
    "I am also creating horizontal and vertical flips as well as providing random zooms between 80% and 120% in order to generate more data. The given 1800 images is a considerably small dataset.\n",
    "\n",
    "With The Validation Data I am only rescaling since that is the only transformation I will be performing on test (or production data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             vertical_flip=True,\n",
    "                             rescale=1./255)\n",
    "\n",
    "train = pd.read_csv('./Train_Test/Train.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val = train[round(len(train)*0.95):]\n",
    "train = train[0:round(len(train)*0.95)]\n",
    "\n",
    "train_gen = datagen.flow_from_dataframe(train, x_col='path', y_col=['healthy', 'multiple_diseases', 'rust', 'scab'], \n",
    "                                        class_mode='raw', target_size=(224, 224), batch_size=64)\n",
    "val_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(val, x_col='path', y_col=['healthy', 'multiple_diseases', 'rust', 'scab'], \n",
    "                                                                class_mode='raw', target_size=(224, 224), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback and Train:\n",
    "\n",
    "When fitting deep networks the learning rate will need to be adjusted otherwise you hover around the minimum without ever converging. \n",
    "\n",
    "I am using Keras ReduceLROnPlateau to handle this for me. \n",
    "\n",
    "Basically, after the validation score doesn't improve for 5 epochs the training rate will lower so more \"fine\" tuning can be done\n",
    "\n",
    "Afterwards I am fitting for 150 Epochs. (This number is mostly arbitrary. Another Callback could be assigned to perform Early Stopping with patience of say \"20\") But I am just running this as a preliminary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "\n",
    "\n",
    "resmodel.fit(train_gen, validation_data=val_gen, epochs=150, callbacks=[rlrop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test:\n",
    "\n",
    "After the model is finished training we run predictions on the test DataSet. \n",
    "\n",
    "The main differences here are to set ```Shuffle= False``` Otherwise the data won't be in the same order as we passed it in and, ```class_mode=None```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_dataframe(dataframe= pd.read_csv('./Train_Test/Test.csv').drop('Unnamed: 0', axis=1),\n",
    "                                                                  x_col=\"path\",\n",
    "                                                                  y_col=None,\n",
    "                                                                  batch_size=64,\n",
    "                                                                  shuffle=False,\n",
    "                                                                  class_mode=None,\n",
    "                                                                  target_size=(224,224))\n",
    "\n",
    "preds = resmodel.predict(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission:\n",
    "\n",
    "Building the submission (Remember to set your index to False when finishing the csv)\n",
    "\n",
    "The rest of this section is just some simple Pandas and Numpy to structure the data.\n",
    "\n",
    "Rememeber if you want to look at what any of these objects look like just create a new cell and run it with the object (example: df) within it and it will print below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./Train_Test/Test.csv').drop('Unnamed: 0', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_pre = test.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_pre.drop('path', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_named = subs_pre.rename(columns={0:'healthy', 1:'multiple_diseases', 2:'rust', 3:'scab'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "subs_named['healthy'] = np.where(subs_named['healthy'] > 0.75, 1., 0.0)\n",
    "subs_named['multiple_diseases'] = np.where(subs_named['multiple_diseases'] > 0.75, 1., 0.0)\n",
    "subs_named['rust'] = np.where(subs_named['rust'] > 0.75, 1., 0.0)\n",
    "subs_named['scab'] = np.where(subs_named['scab'] > 0.75, 1., 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_named.to_csv('./first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
